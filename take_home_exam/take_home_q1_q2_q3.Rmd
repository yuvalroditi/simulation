---
title: '311306435'
output: pdf_document
date: '2023-07-18'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

install.packages("DirichletReg")
install.packages("scatterplot3d") # Install
install.packages("MASS")
install.packages("MCMCprecision")
install.packages("ggpubr")

library(gridExtra)
library(ggpubr)
library(cowplot)
require(MCMCprecision)
library(Formula)
library(DirichletReg)
library("scatterplot3d") # load
library(MASS)
```


Question 1

```{r Question 1, echo=FALSE}
objective <- function(observations, rates, alphas){
  log_likelihood <- sum(ddirichlet(observations, alphas, log = TRUE))
  log_priors <- sum(dexp(rates, log = TRUE))
  log_posterior <- log_likelihood + log_priors
  return (log_posterior)
}
  
obj_hessian <- function(alphas, n){
  k <- length(alphas)
  h <- (-n)*(diag(trigamma(alphas), nrow = k, ncol = k) + 
        matrix(-trigamma(sum(alphas)), nrow = k, ncol = k))  
  return (h)
}

grad <- function(observation, rates, alphas){
  n <- dim(observation)[1]
  log_xj <- colSums(log(observation))
  log_rates <- log(rates)
  return(log_xj - log_rates -n * (digamma(alphas)-digamma(sum(alphas))))
}

opt <- function(observations, rates, tol = 0.001, max_iter = 1000, step=0.05){
  alpha_t <- fit_dirichlet(observations)$alpha
  f_prev <- objective(observations, rates, alpha_t) 
  diff <- Inf
  for(i in 1:max_iter){
    if (diff > tol){
      n <- dim(observations)[1]
      H <- obj_hessian(alpha_t, n)   
      g <- grad(observations, rates, alpha_t)
      H_inv <- solve(H)
      alpha_t <- as.vector(alpha_t - H_inv %*% g)
      f_new <- objective(observations, rates, alpha_t)
      diff <- abs(f_new - f_prev)
      f_prev <- f_new
    } else {
      return (alpha_t)
    }
  }
}

normal_est <- function(observations, rates){
  alphas <- opt(observations, rates)
  mean <- alphas
  n <- dim(observations)[1]
  h <- obj_hessian(alphas, n)
  cov_mat <- ginv(-h)
  return(list(mean = mean, sigma = cov_mat, hess = h))
}


q1 <-function(n, observations, rates){
  estimators <- normal_est(observations, rates)
  mean <- estimators$mean
  k <- length(mean)
  cov_mat <- estimators$sigma
  #cov_mat <- matrix(unlist(estimators[2]), ncol = k, nrow=k)
  samples <- mvrnorm(n, mean, cov_mat)
  return (samples)
}
```

Question 2
```{r Question 2, echo=FALSE}
q2 <- function(n, x, delta,
                           n.initial.c = 100,
                           max.iter = 10^8
                           ){
  # Estimating the mu and Sigma for the Gaussian proposals
  k <- length(delta)
  lower <- numeric(k)
  upper <- numeric(k) + Inf
  out <- normal_est(x, delta)
  mu <- out$mean
  Sigma <- out$sigma
  P <- -out$hess
  d <- 0.5*log(det(P)) - log(2*pi)
  
  # The (log) Gaussian density
  normal.log.g <- function(alpha, mu, P, d){
    -0.5 * drop(t(alpha-mu) %*% P %*% (alpha-mu)) + d
  }

  # Producing an initial estimate of c
  al <- mvrnorm(n.initial.c,mu,Sigma)
  f <- apply(al, 1, function(al) objective(x, delta, al))
  g <- apply(al, 1, function(al) normal.log.g(al,mu,P,d))
  c.est <- initial.c <- max(f - g)

  Alpha <- matrix(NA,n,k)
  n.iter <- 0
  for(i in 1:n){
    if (i%%100 == 0){
      print(i)
    }
    accept <- FALSE
    while((!accept) & (n.iter < max.iter)){
      al <- mvrnorm(1,mu,Sigma)
      if (all(al > lower) & all(al < upper)){
        f <- objective(x, delta, al)
        g <- normal.log.g(al,mu,P,d)
        e <- -rexp(1)
        accept <- e <= f - g - c.est
        c.est <- max(c.est, f - g)
      }
      n.iter <- n.iter + 1
      if (n.iter %% round(max.iter/4) == 0) print (c(n.iter,i,c.est))
      if (n.iter >= max.iter)
        stop(paste("Reached max.ietr = ", max.iter,
                    ". Number of simulations = ", i, sep = ""))
    }
    Alpha[i,] <- al
  }
  c.est = c(initial.c, c.est)
  names(c.est) <- c("initial", "final")
  return(list(Alpha = Alpha,
              c.est = c.est,
              n.iter = n.iter))
}
```

Question 3
```{r Visual comparsion for k=2, echo = FALSE}
for (x_size in list(1000,2000,4000)){
  rates <- c(1,2)
  X <- rdirichlet(x_size, rates)
  q1_alphas <- q1(50, X, rates)
  q2_alphas <- q2(50, X, rates)$Alpha
  
  x_range<- range(c(q1_alphas[,1], q2_alphas[,1]))
  x_range[1] <- x_range[1] - 0.01
  x_range[2] <- x_range[2] + 0.01
  y_range<- range(c(q1_alphas[,2], q2_alphas[,2]))
  y_range[1] <- y_range[1] - 0.01
  y_range[2] <- y_range[2] + 0.01
  par(mfrow=c(1,2), bg="#FFFFC8")
  image(kde2d(q1_alphas[,1],q1_alphas[,2], n = 50),
        main= paste("Gaussian approximation - Task1\n", "X size:", x_size),
        cex.main= 1,
        xlim=x_range,
        ylim=y_range,
        )
  image(kde2d(q2_alphas[,1],q2_alphas[,2], n = 50), 
        main= paste("Rejection Method approximation - Task2\n", "X size:", x_size),
        cex.main= 1,
        xlim=x_range,
        ylim=y_range)
}
```

```{r visual comparsion for k=3, echo = FALSE}
install.packages("ggplot2")
library(ggplot2)
library(ggpubr)

# Create the first ggplot
library(ggplot2)
install.packages("gridExtra")  # Install the package (only needed once)
library(gridExtra)

install.packages("plyr")
library(plyr)

for (x_size in list(100,1000,10000)){
  rates <- c(1,3,5)
  X <- rdirichlet(x_size, rates)
  q1_alphas <- q1(50, X, rates)
  q2_alphas <- q2(50, X, rates)$Alpha
  q1_df <- data.frame(q1_alphas)
  q1_df$approximation <- "Guassian_Task1"
  q2_df <- data.frame(q2_alphas)
  q2_df$approximation <- "Rejection_Task2"
  df <- rbind(q1_df, q2_df)
  
  mu1 <- ddply(df, "approximation", summarise, grp.mean=mean(X1))
  p1 <- ggplot(df, aes(x = X1, color=approximation, fill=approximation)) +
    geom_histogram(alpha=0.3) +
    geom_density(alpha=0.1)+
    geom_vline(data=mu1, aes(xintercept=grp.mean, color=approximation), linetype="dashed") + 
     theme(aspect.ratio = 1) +
    labs(title = paste("Alpha1, X size:",x_size))

  mu2 <- ddply(df, "approximation", summarise, grp.mean=mean(X2))
  p2 <- ggplot(df, aes(x = X2, color=approximation, fill=approximation)) +
    geom_histogram(alpha=0.3) +
    geom_density(alpha=0.1)+
    geom_vline(data=mu2, aes(xintercept=grp.mean, color=approximation), linetype="dashed") + 
    theme(aspect.ratio = 1) +
    labs(title = paste("Alpha2, X size:",x_size))

  mu3 <- ddply(df, "approximation", summarise, grp.mean=mean(X3))
  p3 <- ggplot(df, aes(x = X3, color=approximation, fill=approximation)) +
    geom_histogram(alpha=0.3) +
    geom_density(alpha=0.1)+
    geom_vline(data=mu3, aes(xintercept=grp.mean, color=approximation), linetype="dashed") + 
    theme(aspect.ratio = 1) +
    labs(title = paste("Alpha3, X size:",x_size))
  combined <- ggarrange(p1, p2, p3, ncol=3, nrow=1, common.legend = TRUE, legend="bottom")
  print(combined)
}
```
```{r numerical comparsion}
samples_sizes <- list(64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536)
ks_results <- matrix(ncol = 0, nrow=length(samples_sizes))
for (k in 2:5){
  print(k)
  results_per_k <- c()
  for (x_size in samples_sizes){
    print(paste("x_size", x_size))
    rates <- numeric(k)+5
    X <- rdirichlet(x_size, rates)
    q1_alphas <- q1(200, X, rates)
    q2_alphas <- q2(200, X, rates)$Alpha
    
    ks_result <- ks.test(q1_alphas, q2_alphas)
    results_per_k <- append(results_per_k, ks_result$p.value)
  }
  ks_results <- cbind(ks_results, results_per_k)  
}
print(ks_results)
```
```{r}
log_ks_results <- log10(ks_results)
log_ks_results <- replace(log_ks_results, is.infinite(log_ks_results) & log_ks_results < 0, -16)
# Generate x values (optional, assuming the matrix represents time points)
x_values <- 1:nrow(log_ks_results)

# Plot all columns in the matrix as line plots
matplot(x_values, log_ks_results, type = "l", lty = 1, col = 1:ncol(log_ks_results), lwd = 2,
        xlab = "X-axis Label", ylab = "Y-axis Label", main = "Line Plots for Matrix Columns")

# Add a legend to the plot
legend("bottomright", legend = paste("k=",seq(2,5)), col = 1:ncol(log_ks_results), lty = 1, lwd = 2)
```
Question 4
```{r read data, echo = FALSE}
library(tidyr)
Q3 <- as.matrix(read.table("Q3.txt.Q", sep=" "))
Q4 <- as.matrix(read.table("Q4.txt.Q", sep=" "))
# df_3 <- data.frame(Q3)
# df <- gather(df, key="population", value="proportion")
```


```{r count non zero}
Q3_non_zeros <- rowSums(Q3 >= 0.001)
Q4_non_zeros <- rowSums(Q4 >= 0.001)
q3_labels <- round(table(Q3_non_zeros)/length(Q3_non_zeros)*100)
q4_labels <- round(table(Q4_non_zeros)/length(Q4_non_zeros)*100)
print(q3_labels)
print(q4_labels)
par(mfrow=c(1,2))
pie(table(Q3_non_zeros), main="Q3 non zero elements count", labels = q3_labels)
pie(table(Q4_non_zeros), main="Q4 non zero elements count", labels = q4_labels)
```
We can see that in both cases the samples are affected by one component of the distribution, although there are 3 or 4 components. Its look like adding the fourth component did not improved the explainability of the data that much.
```{r finds similar groups}
# Decide how many clusters to look at
n_clusters <- 10

# Initialize total within sum of squares error: wss
wss_Q3 <- matrix(nrow=n_clusters, ncol=3)
wss_Q4 <- matrix(nrow=n_clusters, ncol=3)

set.seed(123)

# Look over 1 to n possible clusters
for (i in 1:n_clusters) {
  print(i)
  # Fit the model: km.out
  km_out_Q3 <- kmeans(Q3, centers = i, nstart = 100)
  km_out_Q4 <- kmeans(Q4, centers = i, nstart = 100)
  # Save the within cluster sum of squares
  wss_Q3[i,1] <- i
  wss_Q4[i,1] <- i
  wss_Q3[i,2] <- km_out_Q3$tot.withinss
  wss_Q4[i,2] <- km_out_Q4$tot.withinss
  wss_Q3[i,3] <- "Q3"
  wss_Q4[i,3] <- "Q4"
}

# Produce a scree plot
concated_matrix <- rbind(wss_Q3, wss_Q4)
wss_df <- data.frame(clusters = as.integer(concated_matrix[,1]),
                     wss = as.integer(concated_matrix[,2]),
                     matrix = concated_matrix[,3])
scree_plot <- ggplot(wss_df, aes(x = clusters, y = wss, color = matrix)) +
    geom_point(size = 4)+
    geom_line() +
    scale_x_continuous(breaks = c(2, 4, 6, 8, 10)) +
    xlab('Number of clusters')
scree_plot
```
The goal of that plot is to check what is the best k for k-means algorithm. By the elbow method, its look like it is best to select k=3 for Q3 and k=4 for Q4. By choosing k=8 to k=10 the loss is almost 0.
```{r}
install.packages('ggtern')
library('ggtern')
for (c in list(3,10)){
  clusters <- kmeans(Q3, centers = c, nstart = 100)$cluster
  clusters <- as.factor(clusters)
  Q3_clusters_df <- data.frame(Q3, clusters)
  g <- ggtern(data = Q3_clusters_df, aes(x = V1, y = V2, z = V3, color = clusters)) +
    geom_point() +
    labs(title = paste("K-means clustering with", toString(c) ,"clusters and 3 components of Dirichlet distribution"),
         x = "Population 1",
         y = "Population 2",
         z = "Population 3",
         color = "Cluster")  
  print(g)
}
```
By looking at the trinary graph, its look like there are few different groups with different behaviors and distributions. Although the elbow method suggets to take for k-means k=3, I think it is much more suitable to take k=10. In that way, groups with no contribution in one of the components will attributed to separate cluster than those of affected from all the components like the points in the middle of the triangle. Yet, using k-means is not optimal as the centers are spatial and they separate gorups that suppose to be together like the ones in the left upper edge. 

```{r distance matrix}
# install.packages("factoextra")
# install.packages("cluster")
# install.packages("magrittr")
# install.packages("dendextend")
# 
# library("cluster")
# library("factoextra")
# library("magrittr")
# library("dendextend")
# # res.dist <- get_dist(Q3, stand = TRUE, method = "pearson")
# 
# # fviz_dist(res.dist, 
# #    gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
# ```
# ```{r}
# # res.hc <- Q3 %>%
# #   dist(method = "euclidean") %>% # Compute dissimilarity matrix
# #   hclust(method = "ward.D2")     # Compute hierachical clustering
# # 
# # # Visualize using factoextra
# # # Cut in 4 groups and color by groups
# # fviz_dend(res.hc, k = 10, # Cut in four groups
# #           cex = 0.5, # label size
# #           k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
# #           color_labels_by_k = TRUE, # color labels by groups
# #           rect = TRUE # Add rectangle around groups
# #           )
```
```{r}
install.packages("fpc")
install.packages("dbscan")
library("fpc")
library("dbscan")
set.seed(123)
db <- fpc::dbscan(Q3, eps = 0.1, MinPts = 5)

# Plot DBSCAN results
library("factoextra")
fviz_cluster(db, data = Q3, stand = FALSE,
             ellipse = FALSE, show.clust.cent = FALSE,
             geom = "point",palette = "jco", ggtheme = theme_classic())
```
In this section we try to use DBSCAN in order to find better clusters. The assumption of DBSCAN is that clusters are densed in the space more than other areas in the space. It groups densely grouped data points into single cluster. It performs better on separating the edges to other groups than the points that affected from 2-3 components.
```{r spectral clustering}
install.packages("kernlab")
library(kernlab)
set.seed(127) 
clusters <- specc(Q3, centers = 8, scale=TRUE, kernel="splinedot")
clusters <- as.factor(clusters)
Q3_clusters_df <- data.frame(Q3, clusters)
g <- ggtern(data = Q3_clusters_df, aes(x = V1, y = V2, z = V3, color = clusters)) +
  geom_point() +
  labs(title = "Spectral clustering with splinedot kernel",
       x = "Population 1",
       y = "Population 2",
       z = "Population 3",
       color = "Cluster")  
print(g)
```
In this section we apply spectral clustering with splinedot kernel. The goal was to find a kernel that separates the best between the zero components group to the the groups that are contributed from all the components. There are real 5 groups (and 3 more groups that catched single dots). This is the best clusters we got up to this point. 

```{r}
par(mfrow=c(1,3))
#cluster 1
x<- Q3_clusters_df[Q3_clusters_df$clusters == 1, ]$V2
y<- Q3_clusters_df[Q3_clusters_df$clusters == 1, ]$V3
image(kde2d(x,y, n = 50), main= paste("Cluster 1"), cex.main= 1)

#cluster 2
x<- Q3_clusters_df[Q3_clusters_df$clusters == 2, ]$V1
y<- Q3_clusters_df[Q3_clusters_df$clusters == 2, ]$V3
image(kde2d(x,y, n = 50), main= paste("Cluster 2"), cex.main= 1)

#cluster 4
x<- Q3_clusters_df[Q3_clusters_df$clusters == 4, ]$V1
y<- Q3_clusters_df[Q3_clusters_df$clusters == 4, ]$V2
image(kde2d(x,y, n = 50), main= paste("Cluster 4"), cex.main= 1)
```

```{r}

#cluster 3
x<- Q3_clusters_df[Q3_clusters_df$clusters == 3, ]$V1
y<- Q3_clusters_df[Q3_clusters_df$clusters == 3, ]$V2
z<- Q3_clusters_df[Q3_clusters_df$clusters == 3, ]$V3
scatterplot3d(x, y, z, color = "blue", pch = 16, main = "3D Scatter of cluster 3",
              xlab = "population 1", ylab = "population 2", zlab = "population 3")
```
By examine the cluster, we can find 3 kinds of clusters:
1. One component cluster - affected by one component only (Alpha3 = 0.99998 for example)
2. Two components cluster. We can see that it is approximately Guassian by looking at the heatmap
3. Three components cluster. We can see that it is approximately Guassian by looking at the 3DScatterplot

To sum up we had 
Cluster 5, 8 - one component 
Clusters 1, 2, 4, 7(small) - two components, approximately Guassian distributed
cluster 3 - three components, approximately Guassian distributed

So we can say that Q3 composed from 5 major groups, each from specific as mentioned above. 

```{r}
install.packages("GGally")
library(GGally)

# Generate some sample data with four variables
set.seed(123)
n <- 100
data <- data.frame(
  var1 = rnorm(n),
  var2 = rnorm(n),
  var3 = rnorm(n),
  var4 = rnorm(n)
)

# Create the scatterplot matrix
ggpairs(data)
```
```{r}

# clusters <- as.factor(clusters)
# Q4_clusters_df <- data.frame(Q4, clusters)
```
```{r}
install.packages(c("klaR", "scatterplot3d"))
install.packages("haven")
library(haven)
library(klaR)

library(kernlab)
set.seed(132) 
clusters <- specc(Q4, centers = 8, scale=TRUE, kernel="splinedot")
clusters_fac <- as.factor(clusters)
Q4_clusters_df <- data.frame(Q4, clusters)
colors = c("#FF0000", "#808000", "#00FF00", "#00FFFF", "#008080", "#0000FF", "#FF00FF", "#800080")
par(mfrow=c(2,3))
for (angle in seq(0,150,30)){
  print(angle)
  quadplot(Q4,
           main=paste("spectral clustering angle:",toString(angle)),
           angle = angle,
           labelcol = "black",
           col=colors[c(clusters)],
           pch=19,
           lwd=2,
           cex=1,
           legend.control = list(plot=FALSE))
}

```